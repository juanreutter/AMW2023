{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6487f3ea",
   "metadata": {},
   "source": [
    "Code based on this [tutorial](https://keras.io/examples/graph/gnn_citations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b416bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1998cdd4",
   "metadata": {},
   "source": [
    "## Simple MPNN with two layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d707bb1",
   "metadata": {},
   "source": [
    "### This is how the full MPNN classifier looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffabda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_features,\n",
    "        edges,\n",
    "        num_classes,\n",
    "        hidden_layers = [32,32],\n",
    "        dropout_rate=0.2,\n",
    "        normalize=True,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        super(MPNN, self).__init__(*args, **kwargs)\n",
    "\n",
    "# In this code we store full graph in memory \n",
    "\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "\n",
    "# two MPNN layers\n",
    "\n",
    "        self.layer1 = MPNNlayer(\n",
    "            hidden_layers,\n",
    "            dropout_rate,\n",
    "            name=\"layer1\",\n",
    "        )\n",
    "        # Create the second GraphConv layer.\n",
    "        self.layer2 = MPNNlayer(\n",
    "            hidden_layers,\n",
    "            dropout_rate,\n",
    "            name=\"layer2\",\n",
    "        )\n",
    "        \n",
    "        # Compute logits layer for the classifier (the decoder part)\n",
    "        self.clas = layers.Dense(units=num_classes, name=\"logits\")\n",
    "\n",
    "        \n",
    "    def call(self, batch_indices):\n",
    "         \n",
    "        messages_1 = self.layer1(self.node_features,self.edges)\n",
    "        \n",
    "        messages_2 = self.layer2(messages_1,self.edges)\n",
    "        \n",
    "\n",
    "        ### Now we gather the embeddings in batch_indexes, so that we classify and \n",
    "        ### compare the result only for those nodes.\n",
    "        \n",
    "        batch_node_embeddings = tf.gather(messages_2, batch_indices)\n",
    "\n",
    "        # Readout to get the paper subjects from the embeddings\n",
    "        \n",
    "        return self.clas(batch_node_embeddings)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e9dcee",
   "metadata": {},
   "source": [
    "### Taking a deeper look: first defining our multi-layered perceptrons, then MPNN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca93769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MLP(hidden_layers, dropout_rate, name=None):\n",
    "    mlp = []\n",
    "\n",
    "    for layer in hidden_layers:\n",
    "        mlp.append(layers.BatchNormalization())\n",
    "        mlp.append(layers.Dropout(dropout_rate))\n",
    "        mlp.append(layers.Dense(layer, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(mlp, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873591a",
   "metadata": {},
   "source": [
    "### MPNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd7185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNNlayer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layers = [32,32],\n",
    "        dropout_rate=0.2,\n",
    "        normalize=False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(MPNNlayer, self).__init__(*args, **kwargs)\n",
    "            \n",
    "        ### These are the two trainable parts of the network. \n",
    "        \n",
    "        self.mlp_aggregate = create_MLP(hidden_layers, dropout_rate)\n",
    "\n",
    "        self.mlp_update = create_MLP(hidden_layers, dropout_rate)\n",
    "\n",
    "    def call(self, node_repesentations, edges):\n",
    "\n",
    "        # node_indexes: for each edge, the source node of the edge\n",
    "        # neihbor_indexes: for each edge, the target node of the edge\n",
    "        node_indexes, neighbor_indexes = edges[0], edges[1]\n",
    "        \n",
    "        # neighbour_repesentations: for each edge, the embedding of the target node\n",
    "        neighbour_repesentations = tf.gather(node_repesentations, neighbor_indexes)\n",
    "\n",
    "        ### Now comes computation:\n",
    "        \n",
    "        #Aggregate\n",
    "        aggregated_messages = self.aggregate(\n",
    "            node_indexes, neighbour_repesentations, node_repesentations\n",
    "        )\n",
    "        \n",
    "        #Update\n",
    "        return self.update(node_repesentations, aggregated_messages)\n",
    "\n",
    "    def aggregate(self, node_indexes, neighbour_messages, node_repesentations):\n",
    "        num_nodes = node_repesentations.shape[0]\n",
    "        \n",
    "        ### Run node features through an MLP\n",
    "        \n",
    "        preprocessed_messages = self.mlp_aggregate(neighbour_messages)\n",
    "        \n",
    "        ### The sum is taken care of with tf's unsorted segment sum. \n",
    "        ### First, get all positions of edges where node i is the source. \n",
    "        ### Then, sum all preprocessed messages in this position (they \n",
    "        ### have the embedding of the target of the edge). \n",
    "        ### The sum is put on the i-th entry of aggregated_message\n",
    "\n",
    "        aggregated_message = tf.math.unsorted_segment_sum(\n",
    "            preprocessed_messages, node_indexes, num_segments=num_nodes\n",
    "        )\n",
    "        \n",
    "        return aggregated_message\n",
    "\n",
    "    def update(self, node_repesentations, aggregated_messages):\n",
    "        \n",
    "        ### concatenate aggregated message with my own representation\n",
    "        updated_messages = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
    "        \n",
    "        ### apply trainable MLP to the result\n",
    "        \n",
    "        node_embeddings = self.mlp_update(updated_messages)\n",
    "\n",
    "        return node_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35598f9",
   "metadata": {},
   "source": [
    "## Trying out our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a03400",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Quora dataset\n",
    "citations = pd.read_csv(\"cora.cites\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "\n",
    "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "papers = pd.read_csv(\"cora.content\", sep=\"\\t\", header=None, names=column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe3c4906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1364, 1435)\n",
      "Test data shape: (1344, 1435)\n"
     ]
    }
   ],
   "source": [
    "### Some structuring, cleaning\n",
    "\n",
    "class_values = sorted(papers[\"subject\"].unique())\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "\n",
    "train_data, test_data = [], []\n",
    "\n",
    "for _, group_data in papers.groupby(\"subject\"):\n",
    "    # Select around 50% of the dataset for training.\n",
    "    random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
    "    train_data.append(group_data[random_selection])\n",
    "    test_data.append(group_data[~random_selection])\n",
    "\n",
    "train_data = pd.concat(train_data).sample(frac=1)\n",
    "test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9712ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>term_0</th>\n",
       "      <th>term_1</th>\n",
       "      <th>term_2</th>\n",
       "      <th>term_3</th>\n",
       "      <th>term_4</th>\n",
       "      <th>term_5</th>\n",
       "      <th>term_6</th>\n",
       "      <th>term_7</th>\n",
       "      <th>term_8</th>\n",
       "      <th>...</th>\n",
       "      <th>term_1424</th>\n",
       "      <th>term_1425</th>\n",
       "      <th>term_1426</th>\n",
       "      <th>term_1427</th>\n",
       "      <th>term_1428</th>\n",
       "      <th>term_1429</th>\n",
       "      <th>term_1430</th>\n",
       "      <th>term_1431</th>\n",
       "      <th>term_1432</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>1590</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>593</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>2406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>2573</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1413</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>1613</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>1059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2338</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1364 rows Ã— 1435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      paper_id  term_0  term_1  term_2  term_3  term_4  term_5  term_6  \\\n",
       "1936      1590       0       0       0       0       0       0       0   \n",
       "2675       593       0       0       0       0       0       0       0   \n",
       "2140      2406       0       0       0       0       0       0       0   \n",
       "2308      2573       0       0       0       0       0       0       0   \n",
       "2560       202       0       0       0       0       1       0       0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "525       1413       0       0       0       0       0       0       0   \n",
       "629        418       0       0       1       0       1       0       0   \n",
       "2152      1613       0       0       0       0       0       0       1   \n",
       "704       1059       0       0       1       0       0       0       0   \n",
       "2338        69       0       0       0       0       0       0       0   \n",
       "\n",
       "      term_7  term_8  ...  term_1424  term_1425  term_1426  term_1427  \\\n",
       "1936       0       0  ...          0          0          0          0   \n",
       "2675       0       0  ...          0          0          0          0   \n",
       "2140       0       1  ...          0          0          0          0   \n",
       "2308       0       0  ...          0          0          0          0   \n",
       "2560       0       0  ...          0          0          0          0   \n",
       "...      ...     ...  ...        ...        ...        ...        ...   \n",
       "525        0       0  ...          0          0          0          0   \n",
       "629        0       0  ...          0          0          0          0   \n",
       "2152       0       0  ...          0          0          0          0   \n",
       "704        0       0  ...          0          0          0          0   \n",
       "2338       0       0  ...          0          0          0          0   \n",
       "\n",
       "      term_1428  term_1429  term_1430  term_1431  term_1432  subject  \n",
       "1936          0          0          0          0          0        1  \n",
       "2675          0          0          0          0          0        6  \n",
       "2140          0          0          0          0          0        3  \n",
       "2308          0          0          0          0          0        0  \n",
       "2560          0          0          0          0          0        6  \n",
       "...         ...        ...        ...        ...        ...      ...  \n",
       "525           0          0          0          0          0        6  \n",
       "629           0          0          0          1          0        0  \n",
       "2152          0          0          0          0          0        1  \n",
       "704           0          0          0          0          0        3  \n",
       "2338          0          0          0          0          0        0  \n",
       "\n",
       "[1364 rows x 1435 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34a6a66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1590,  593, 2406, ..., 1613, 1059,   69]),\n",
       " array([1, 6, 3, ..., 1, 3, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = set(papers.columns) - {\"paper_id\", \"subject\"}\n",
    "num_features = len(feature_names)\n",
    "num_classes = len(class_idx)\n",
    "\n",
    "# Create train and test features as a numpy array.\n",
    "x_train = train_data[\"paper_id\"].to_numpy()\n",
    "x_test = test_data[\"paper_id\"].to_numpy()\n",
    "# Create train and test targets as a numpy array.\n",
    "y_train = train_data[\"subject\"].to_numpy()\n",
    "y_test = test_data[\"subject\"].to_numpy()\n",
    "\n",
    "x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aacaa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape: (2, 5429)\n",
      "Nodes shape: (2708, 1433)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/12lsjk9d5mn0gdyhlk_xd0zm0000gn/T/ipykernel_73683/3546402807.py:6: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n"
     ]
    }
   ],
   "source": [
    "#Adyacency list \n",
    "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
    "\n",
    "# Tensorflow vector for each node\n",
    "node_features = tf.cast(\n",
    "    papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=tf.dtypes.float32\n",
    ")\n",
    "\n",
    "print(\"Edges shape:\", edges.shape)\n",
    "print(\"Nodes shape:\", node_features.shape)\n",
    "\n",
    "# Create the GNN. Note how the entire graph is passed onto the model. \n",
    "\n",
    "GNN = MPNN(\n",
    "    node_features = node_features,\n",
    "    edges = edges,\n",
    "    num_classes=7,\n",
    "    hidden_layers=[32,32],\n",
    "    dropout_rate=0.2,\n",
    "    name=\"mpnn_model\",\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ff100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "5/5 [==============================] - 2s 140ms/step - loss: 2.0577 - acc: 0.2312 - val_loss: 1.9116 - val_acc: 0.1756\n",
      "Epoch 2/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 1.6502 - acc: 0.3701 - val_loss: 1.8202 - val_acc: 0.3024\n",
      "Epoch 3/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 1.3990 - acc: 0.4694 - val_loss: 1.6803 - val_acc: 0.3659\n",
      "Epoch 4/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 1.1751 - acc: 0.5643 - val_loss: 1.9382 - val_acc: 0.3463\n",
      "Epoch 5/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.9449 - acc: 0.6704 - val_loss: 2.6833 - val_acc: 0.3707\n",
      "Epoch 6/300\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.7612 - acc: 0.7403 - val_loss: 2.7967 - val_acc: 0.3951\n",
      "Epoch 7/300\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.5820 - acc: 0.7990 - val_loss: 2.9463 - val_acc: 0.4049\n",
      "Epoch 8/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.4242 - acc: 0.8559 - val_loss: 2.2530 - val_acc: 0.5122\n",
      "Epoch 9/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.3610 - acc: 0.8706 - val_loss: 1.8139 - val_acc: 0.5951\n",
      "Epoch 10/300\n",
      "5/5 [==============================] - 1s 187ms/step - loss: 0.2868 - acc: 0.9016 - val_loss: 2.1812 - val_acc: 0.5707\n",
      "Epoch 11/300\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.2117 - acc: 0.9249 - val_loss: 4.2828 - val_acc: 0.4634\n",
      "Epoch 12/300\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.2023 - acc: 0.9344 - val_loss: 4.8745 - val_acc: 0.4439\n",
      "Epoch 13/300\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.1822 - acc: 0.9500 - val_loss: 3.7052 - val_acc: 0.4927\n",
      "Epoch 14/300\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.1229 - acc: 0.9612 - val_loss: 3.1610 - val_acc: 0.5317\n",
      "Epoch 15/300\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.1086 - acc: 0.9629 - val_loss: 2.4804 - val_acc: 0.5805\n",
      "Epoch 16/300\n",
      "5/5 [==============================] - 0s 102ms/step - loss: 0.0957 - acc: 0.9689 - val_loss: 2.0368 - val_acc: 0.6341\n",
      "Epoch 17/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.1037 - acc: 0.9646 - val_loss: 1.9989 - val_acc: 0.6439\n",
      "Epoch 18/300\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.0864 - acc: 0.9733 - val_loss: 2.8494 - val_acc: 0.5756\n",
      "Epoch 19/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0639 - acc: 0.9810 - val_loss: 3.2411 - val_acc: 0.5707\n",
      "Epoch 20/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0862 - acc: 0.9741 - val_loss: 3.0875 - val_acc: 0.5463\n",
      "Epoch 21/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0839 - acc: 0.9733 - val_loss: 2.3204 - val_acc: 0.5951\n",
      "Epoch 22/300\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0641 - acc: 0.9810 - val_loss: 1.6851 - val_acc: 0.6537\n",
      "Epoch 23/300\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 0.0718 - acc: 0.9793 - val_loss: 1.5301 - val_acc: 0.6927\n",
      "Epoch 24/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0625 - acc: 0.9802 - val_loss: 1.4457 - val_acc: 0.7268\n",
      "Epoch 25/300\n",
      "5/5 [==============================] - 0s 101ms/step - loss: 0.0644 - acc: 0.9793 - val_loss: 1.9165 - val_acc: 0.6829\n",
      "Epoch 26/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0381 - acc: 0.9905 - val_loss: 1.9720 - val_acc: 0.6878\n",
      "Epoch 27/300\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.0436 - acc: 0.9862 - val_loss: 1.5402 - val_acc: 0.7220\n",
      "Epoch 28/300\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.0438 - acc: 0.9853 - val_loss: 1.0529 - val_acc: 0.7561\n",
      "Epoch 29/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0375 - acc: 0.9905 - val_loss: 0.9553 - val_acc: 0.7659\n",
      "Epoch 30/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0524 - acc: 0.9810 - val_loss: 0.9869 - val_acc: 0.7561\n",
      "Epoch 31/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0454 - acc: 0.9879 - val_loss: 1.3130 - val_acc: 0.7171\n",
      "Epoch 32/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0393 - acc: 0.9896 - val_loss: 1.9016 - val_acc: 0.6585\n",
      "Epoch 33/300\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0516 - acc: 0.9827 - val_loss: 2.2383 - val_acc: 0.6488\n",
      "Epoch 34/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0259 - acc: 0.9922 - val_loss: 2.2253 - val_acc: 0.6488\n",
      "Epoch 35/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0417 - acc: 0.9862 - val_loss: 2.2407 - val_acc: 0.6585\n",
      "Epoch 36/300\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.0564 - acc: 0.9836 - val_loss: 1.9111 - val_acc: 0.6976\n",
      "Epoch 37/300\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.0475 - acc: 0.9871 - val_loss: 1.8590 - val_acc: 0.7073\n",
      "Epoch 38/300\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.0444 - acc: 0.9871 - val_loss: 1.8776 - val_acc: 0.7122\n",
      "Epoch 39/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0399 - acc: 0.9845 - val_loss: 1.9213 - val_acc: 0.7220\n",
      "Epoch 40/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0349 - acc: 0.9914 - val_loss: 1.4421 - val_acc: 0.7366\n",
      "Epoch 41/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0419 - acc: 0.9888 - val_loss: 1.3993 - val_acc: 0.7415\n",
      "Epoch 42/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0426 - acc: 0.9896 - val_loss: 1.2780 - val_acc: 0.7610\n",
      "Epoch 43/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0278 - acc: 0.9922 - val_loss: 1.1142 - val_acc: 0.8000\n",
      "Epoch 44/300\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.0279 - acc: 0.9905 - val_loss: 1.0219 - val_acc: 0.8049\n",
      "Epoch 45/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0354 - acc: 0.9914 - val_loss: 1.0067 - val_acc: 0.8049\n",
      "Epoch 46/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0340 - acc: 0.9871 - val_loss: 1.2189 - val_acc: 0.7951\n",
      "Epoch 47/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0220 - acc: 0.9931 - val_loss: 1.2523 - val_acc: 0.7805\n",
      "Epoch 48/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0416 - acc: 0.9879 - val_loss: 1.0859 - val_acc: 0.8195\n",
      "Epoch 49/300\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.0314 - acc: 0.9931 - val_loss: 0.9766 - val_acc: 0.8244\n",
      "Epoch 50/300\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.0130 - acc: 0.9974 - val_loss: 0.9904 - val_acc: 0.8244\n",
      "Epoch 51/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0404 - acc: 0.9896 - val_loss: 1.0033 - val_acc: 0.8146\n",
      "Epoch 52/300\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0335 - acc: 0.9879 - val_loss: 1.0372 - val_acc: 0.8098\n",
      "Epoch 53/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0270 - acc: 0.9922 - val_loss: 0.9847 - val_acc: 0.8098\n",
      "Epoch 54/300\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.0273 - acc: 0.9922 - val_loss: 1.0499 - val_acc: 0.8049\n",
      "Epoch 55/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0321 - acc: 0.9896 - val_loss: 1.1827 - val_acc: 0.8146\n",
      "Epoch 56/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0342 - acc: 0.9905 - val_loss: 1.2190 - val_acc: 0.8000\n",
      "Epoch 57/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0335 - acc: 0.9888 - val_loss: 1.5646 - val_acc: 0.7805\n",
      "Epoch 58/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0300 - acc: 0.9914 - val_loss: 1.1564 - val_acc: 0.7902\n",
      "Epoch 59/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0596 - acc: 0.9888 - val_loss: 1.0226 - val_acc: 0.8146\n",
      "Epoch 60/300\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.0438 - acc: 0.9802 - val_loss: 0.9707 - val_acc: 0.8146\n",
      "Epoch 61/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0302 - acc: 0.9888 - val_loss: 1.0173 - val_acc: 0.8146\n",
      "Epoch 62/300\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.0317 - acc: 0.9905 - val_loss: 0.9632 - val_acc: 0.8146\n",
      "Epoch 63/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 101ms/step - loss: 0.0287 - acc: 0.9871 - val_loss: 0.9615 - val_acc: 0.8293\n",
      "Epoch 64/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0425 - acc: 0.9888 - val_loss: 0.9445 - val_acc: 0.8244\n",
      "Epoch 65/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0155 - acc: 0.9957 - val_loss: 0.9260 - val_acc: 0.8341\n",
      "Epoch 66/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0313 - acc: 0.9888 - val_loss: 0.9770 - val_acc: 0.8244\n",
      "Epoch 67/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0347 - acc: 0.9914 - val_loss: 0.9831 - val_acc: 0.8195\n",
      "Epoch 68/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0322 - acc: 0.9888 - val_loss: 1.0130 - val_acc: 0.8195\n",
      "Epoch 69/300\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.0335 - acc: 0.9888 - val_loss: 1.2117 - val_acc: 0.7902\n",
      "Epoch 70/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0175 - acc: 0.9965 - val_loss: 1.2735 - val_acc: 0.7756\n",
      "Epoch 71/300\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.0480 - acc: 0.9905 - val_loss: 1.2023 - val_acc: 0.8049\n",
      "Epoch 72/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0281 - acc: 0.9914 - val_loss: 1.1744 - val_acc: 0.8000\n",
      "Epoch 73/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0265 - acc: 0.9905 - val_loss: 1.1972 - val_acc: 0.7951\n",
      "Epoch 74/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0360 - acc: 0.9905 - val_loss: 1.2066 - val_acc: 0.8098\n",
      "Epoch 75/300\n",
      "5/5 [==============================] - 1s 94ms/step - loss: 0.0278 - acc: 0.9931 - val_loss: 1.1674 - val_acc: 0.8049\n",
      "Epoch 76/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0285 - acc: 0.9914 - val_loss: 1.1523 - val_acc: 0.8000\n",
      "Epoch 77/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0273 - acc: 0.9896 - val_loss: 1.1660 - val_acc: 0.8000\n",
      "Epoch 78/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0204 - acc: 0.9922 - val_loss: 1.1462 - val_acc: 0.8244\n",
      "Epoch 79/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0177 - acc: 0.9931 - val_loss: 1.2006 - val_acc: 0.8049\n",
      "Epoch 80/300\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.0352 - acc: 0.9888 - val_loss: 1.1989 - val_acc: 0.8146\n",
      "Epoch 81/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0277 - acc: 0.9888 - val_loss: 1.1610 - val_acc: 0.8195\n",
      "Epoch 82/300\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.0094 - acc: 0.9965 - val_loss: 1.1628 - val_acc: 0.8146\n",
      "Epoch 83/300\n",
      "5/5 [==============================] - 1s 129ms/step - loss: 0.0170 - acc: 0.9940 - val_loss: 1.2056 - val_acc: 0.8341\n",
      "Epoch 84/300\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 0.0119 - acc: 0.9957 - val_loss: 1.2568 - val_acc: 0.8195\n",
      "Epoch 85/300\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.0277 - acc: 0.9905 - val_loss: 1.3329 - val_acc: 0.8146\n",
      "Epoch 86/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0211 - acc: 0.9931 - val_loss: 1.4000 - val_acc: 0.8146\n",
      "Epoch 87/300\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.0354 - acc: 0.9922 - val_loss: 1.3774 - val_acc: 0.8195\n",
      "Epoch 88/300\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.0165 - acc: 0.9922 - val_loss: 1.3533 - val_acc: 0.8098\n",
      "Epoch 89/300\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.0295 - acc: 0.9905 - val_loss: 1.2771 - val_acc: 0.8244\n",
      "Epoch 90/300\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.0198 - acc: 0.9940 - val_loss: 1.2703 - val_acc: 0.8195\n",
      "Epoch 91/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0268 - acc: 0.9905 - val_loss: 1.2843 - val_acc: 0.8049\n",
      "Epoch 92/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0186 - acc: 0.9940 - val_loss: 1.1796 - val_acc: 0.8195\n",
      "Epoch 93/300\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.0096 - acc: 0.9974 - val_loss: 1.1665 - val_acc: 0.8195\n",
      "Epoch 94/300\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.0133 - acc: 0.9948 - val_loss: 1.2048 - val_acc: 0.8049\n",
      "Epoch 95/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0340 - acc: 0.9905 - val_loss: 1.1987 - val_acc: 0.8146\n",
      "Epoch 96/300\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.0141 - acc: 0.9931 - val_loss: 1.1606 - val_acc: 0.8000\n",
      "Epoch 97/300\n",
      "5/5 [==============================] - 0s 103ms/step - loss: 0.0142 - acc: 0.9974 - val_loss: 1.1535 - val_acc: 0.8098\n",
      "Epoch 98/300\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.0177 - acc: 0.9957 - val_loss: 1.1597 - val_acc: 0.8146\n",
      "Epoch 99/300\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.0184 - acc: 0.9940 - val_loss: 1.2052 - val_acc: 0.8146\n",
      "Epoch 100/300\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.0166 - acc: 0.9948 - val_loss: 1.2330 - val_acc: 0.8293\n",
      "Epoch 101/300\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.0322 - acc: 0.9905 - val_loss: 1.2709 - val_acc: 0.8195\n",
      "Epoch 102/300\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.0219 - acc: 0.9922 - val_loss: 1.2466 - val_acc: 0.8244\n",
      "Epoch 103/300\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.0204 - acc: 0.9914 - val_loss: 1.3252 - val_acc: 0.8293\n",
      "Epoch 104/300\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.0160 - acc: 0.9965 - val_loss: 1.4679 - val_acc: 0.8049\n",
      "Epoch 105/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0163 - acc: 0.9940 - val_loss: 1.5064 - val_acc: 0.8000\n",
      "Epoch 106/300\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.0267 - acc: 0.9922 - val_loss: 1.4707 - val_acc: 0.8049\n",
      "Epoch 107/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0134 - acc: 0.9948 - val_loss: 1.4462 - val_acc: 0.8000\n",
      "Epoch 108/300\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0176 - acc: 0.9948 - val_loss: 1.4224 - val_acc: 0.8049\n",
      "Epoch 109/300\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.0136 - acc: 0.9948 - val_loss: 1.4789 - val_acc: 0.8146\n",
      "Epoch 110/300\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.0104 - acc: 0.9957 - val_loss: 1.7611 - val_acc: 0.7951\n",
      "Epoch 111/300\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.0213 - acc: 0.9948 - val_loss: 1.7136 - val_acc: 0.8000\n",
      "Epoch 112/300\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.0148 - acc: 0.9957 - val_loss: 1.5853 - val_acc: 0.8049\n",
      "Epoch 113/300\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.0224 - acc: 0.9931 - val_loss: 1.7213 - val_acc: 0.7902\n",
      "Epoch 114/300\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.0127 - acc: 0.9948 - val_loss: 1.8171 - val_acc: 0.7854\n",
      "Epoch 115/300\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.0191 - acc: 0.9957 - val_loss: 1.6830 - val_acc: 0.7951\n"
     ]
    }
   ],
   "source": [
    "GNN.compile(\n",
    "        optimizer=keras.optimizers.Adam(0.01),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_acc\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # Fit the model.\n",
    "history = GNN.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        epochs=300,\n",
    "        batch_size=256,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6865b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 1s 16ms/step - loss: 1.1346 - acc: 0.8266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1346373558044434, 0.8266369104385376]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GNN.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed546713",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87554f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
